#!/usr/bin/env python3
"""
run.py â€” YouTube Shorts â†’ speaker-separated Markdown (Obsidian)

â€¢ WhisperX 3.3.4 ã§ ASR â†’ align
â€¢ pyannote/speaker-diarization-3.1 ã§ 2-4 åã‚’è‡ªå‹•æ¨å®š
â€¢ ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã¨å¾Œå‡¦ç†ã®æ”¹è‰¯ç‰ˆ
"""

from __future__ import annotations
import sys, re, tempfile, pathlib, datetime, yaml, os
from typing import Optional, Tuple, Dict, Any, List
import warnings
warnings.filterwarnings("ignore")

# â”€â”€â”€ 1. è¨­å®š â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
CFG_FILE = "config.yaml"
DEFAULT  = dict(
    vault_path    = "~/ObsidianVault",
    subdir        = "",
    whisper_model = "large-v3",
    language      = "auto",
    hf_token      = "",
    min_segment_duration = 0.20,  # æœ€å°ã‚»ã‚°ãƒ¡ãƒ³ãƒˆé•·ï¼ˆç§’ï¼‰
    merge_gap_threshold  = 0.30   # åŒä¸€è©±è€…ã®ç™ºè©±é–“éš”ã—ãã„å€¤ï¼ˆç§’ï¼‰
)

def load_config():
    """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ãƒ»ä½œæˆ"""
    p_cfg = pathlib.Path(CFG_FILE)
    if not p_cfg.exists():
        p_cfg.write_text(yaml.dump(DEFAULT, allow_unicode=True, default_flow_style=False), encoding="utf-8")
        print("ğŸ”§ config.yaml ã‚’ä½œæˆã—ã¾ã—ãŸã€‚HF_TOKENã‚’è¨­å®šã—ã¦å†å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
        print("   è©±è€…åˆ†é›¢ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã¯ã€Hugging Face ãƒˆãƒ¼ã‚¯ãƒ³ãŒå¿…è¦ã§ã™ã€‚")
        sys.exit(0)
    
    try:
        cfg = {**DEFAULT, **yaml.safe_load(p_cfg.read_text(encoding="utf-8"))}
        return cfg
    except Exception as e:
        print(f"âŒ config.yaml ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {e}")
        sys.exit(1)

cfg = load_config()
VAULT     = pathlib.Path(cfg["vault_path"]).expanduser()
VAULT.mkdir(parents=True, exist_ok=True)
TARGET    = VAULT / cfg["subdir"] if cfg["subdir"] else VAULT
TARGET.mkdir(parents=True, exist_ok=True)
MODEL     = cfg["whisper_model"]
LANG_OPT  = None if cfg["language"] in ("auto", "", None) else cfg["language"]
HF_TOKEN  = cfg.get("hf_token") or os.getenv("HF_TOKEN")
MIN_SPK, MAX_SPK = 2, 4
MIN_SEGMENT_DUR = cfg.get("min_segment_duration", 0.20)
MERGE_GAP = cfg.get("merge_gap_threshold", 0.30)

# â”€â”€â”€ 2. YouTube â†’ wav â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def download_wav(url: str, out_dir: pathlib.Path) -> Tuple[pathlib.Path, Dict[str, Any]]:
    """YouTubeå‹•ç”»ã‹ã‚‰éŸ³å£°ã‚’WAVå½¢å¼ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"""
    try:
        from yt_dlp import YoutubeDL
    except ImportError:
        print("âŒ yt-dlp ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: pip install yt-dlp")
        sys.exit(1)
    
    ydl_opts = {
        'format': 'bestaudio/best',
        'outtmpl': f"{out_dir}/%(id)s.%(ext)s",
        'postprocessors': [{
            'key': 'FFmpegExtractAudio',
            'preferredcodec': 'wav',
            'preferredquality': '192',
        }],
        'quiet': True,
        'noprogress': True,
        'no_warnings': True
    }
    
    try:
        with YoutubeDL(ydl_opts) as ydl:
            info = ydl.extract_info(url, download=True)
        
        if not info:
            raise Exception("å‹•ç”»æƒ…å ±ã®å–å¾—ã«å¤±æ•—")
            
        wav_file = out_dir / f"{info['id']}.wav"
        if not wav_file.exists():
            raise Exception("WAVãƒ•ã‚¡ã‚¤ãƒ«ã®ç”Ÿæˆã«å¤±æ•—")
            
        return wav_file, info
        
    except Exception as e:
        print(f"âŒ éŸ³å£°ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
        print("   URLã‚’ç¢ºèªã™ã‚‹ã‹ã€ffmpegãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        sys.exit(1)

# â”€â”€â”€ 3. WhisperX ASR + Alignment â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def load_whisperx():
    """WhisperXãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿"""
    try:
        import torch
        import whisperx
    except ImportError:
        print("âŒ whisperx ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“:")
        print("   pip install git+https://github.com/m-bain/whisperX.git@main")
        sys.exit(1)
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    compute_type = "float16" if device == "cuda" else "int8"
    
    try:
        model = whisperx.load_model(MODEL, device, compute_type=compute_type)
        print(f"ğŸ™  WhisperX model loaded: {MODEL} ({device})")
        return model, device
    except Exception as e:
        print(f"âŒ WhisperX ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        print(f"   ãƒ¢ãƒ‡ãƒ« '{MODEL}' ãŒè¦‹ã¤ã‹ã‚‰ãªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
        sys.exit(1)

def transcribe_and_align(wav_file: pathlib.Path, language: Optional[str] = None) -> Dict[str, Any]:
    """éŸ³å£°èªè­˜ã¨ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆå®Ÿè¡Œ"""
    try:
        import whisperx
        import torch
    except ImportError:
        print("âŒ whisperx ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«å¤±æ•—")
        sys.exit(1)
    
    try:
        # éŸ³å£°èª­ã¿è¾¼ã¿
        audio = whisperx.load_audio(str(wav_file))
        
        # ASRãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ãƒ»è»¢å†™
        asr_model, device = load_whisperx()
        result = asr_model.transcribe(audio, language=language, batch_size=16)
        
        detected_lang = result.get("language", "en")
        print(f"ğŸŒ æ¤œå‡ºè¨€èª: {detected_lang}")
        
        # ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ãƒ»å®Ÿè¡Œ
        try:
            align_model, metadata = whisperx.load_align_model(
                language_code=detected_lang, 
                device=device
            )
            result = whisperx.align(result["segments"], align_model, metadata, audio, device=device)
            print("âœ“ éŸ³å£°èªè­˜ãƒ»ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆå®Œäº†")
        except Exception as e:
            print(f"âš ï¸  ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆå¤±æ•—: {e}")
            print("   åŸºæœ¬çš„ãªè»¢å†™çµæœã®ã¿ä½¿ç”¨ã—ã¾ã™")
        
        return result
        
    except Exception as e:
        print(f"âŒ éŸ³å£°èªè­˜ã‚¨ãƒ©ãƒ¼: {e}")
        sys.exit(1)

# â”€â”€â”€ 4. Speaker Diarization â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def diarize_audio(wav_file: pathlib.Path):
    """è©±è€…åˆ†é›¢ã®å®Ÿè¡Œ"""
    if not HF_TOKEN:
        print("âš ï¸  HF_TOKEN ãŒè¨­å®šã•ã‚Œã¦ã„ãªã„ãŸã‚è©±è€…åˆ†é›¢ã‚’ã‚¹ã‚­ãƒƒãƒ—")
        return None
    
    try:
        from pyannote.audio import Pipeline
        import torch
    except ImportError:
        print("âŒ pyannote.audio ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: pip install pyannote.audio")
        return None
    
    try:
        # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³èª­ã¿è¾¼ã¿
        pipeline = Pipeline.from_pretrained(
            "pyannote/speaker-diarization-3.1",
            use_auth_token=HF_TOKEN
        )
        
        # GPUä½¿ç”¨å¯èƒ½æ™‚ã¯GPUã«ç§»å‹•
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        pipeline.to(device)
        
        print(f"ğŸ‘¥ è©±è€…åˆ†é›¢å®Ÿè¡Œä¸­... ({device})")
        
        # è©±è€…åˆ†é›¢å®Ÿè¡Œï¼ˆä¼šè©±ç”¨ã«æœ€é©åŒ–ï¼‰
        diarization = pipeline(
            str(wav_file),
            min_speakers=4,  # 4äººã®è©±è€…ã‚’å¼·åˆ¶
            max_speakers=4,  # 4äººã®è©±è€…ã«å›ºå®š
            num_speakers=4,  # è©±è€…æ•°ã‚’æ˜ç¤ºçš„ã«æŒ‡å®š
        )
        
        # å¾Œå‡¦ç†: çŸ­ã„ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®çµ±åˆ
        diarization = smooth_diarization(diarization)
        
        print("âœ“ è©±è€…åˆ†é›¢å®Œäº†")
        return diarization
        
    except Exception as e:
        print(f"âš ï¸  è©±è€…åˆ†é›¢å¤±æ•—: {e}")
        print("   HF_TOKENãŒæ­£ã—ã„ã‹ã€ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶šã‚’ç¢ºèªã—ã¦ãã ã•ã„")
        return None

def smooth_diarization(diarization, min_duration: float = None, merge_gap: float = None):
    """è©±è€…åˆ†é›¢çµæœã®å¾Œå‡¦ç†ï¼ˆçŸ­ã„ã‚»ã‚°ãƒ¡ãƒ³ãƒˆçµ±åˆãƒ»ã‚®ãƒ£ãƒƒãƒ—åŸ‹ã‚ï¼‰"""
    if min_duration is None:
        min_duration = 0.1  # ã‚ˆã‚ŠçŸ­ã„ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’è¨±å®¹
    if merge_gap is None:
        merge_gap = 0.2  # ã‚ˆã‚ŠçŸ­ã„ã‚®ãƒ£ãƒƒãƒ—ã‚’è¨±å®¹
    
    # çŸ­ã„ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’å‰å¾Œã®é•·ã„ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã«çµ±åˆ
    segments = []
    for segment, track, label in diarization.itertracks(yield_label=True):
        segments.append((segment, track, label))
    
    # è©±è€…ã”ã¨ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
    speaker_segments = {}
    for segment, track, label in segments:
        if label not in speaker_segments:
            speaker_segments[label] = []
        speaker_segments[label].append((segment, track))
    
    # å„è©±è€…ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’æ™‚é–“é †ã«ã‚½ãƒ¼ãƒˆ
    for label in speaker_segments:
        speaker_segments[label].sort(key=lambda x: x[0].start)
    
    # ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®çµ±åˆ
    filtered_segments = []
    for label, segs in speaker_segments.items():
        current_seg = None
        current_track = None
        
        for segment, track in segs:
            if current_seg is None:
                current_seg = segment
                current_track = track
            else:
                # ã‚®ãƒ£ãƒƒãƒ—ãŒé–¾å€¤ä»¥ä¸‹ã®å ´åˆã®ã¿çµ±åˆ
                if segment.start - current_seg.end <= merge_gap:
                    from pyannote.core import Segment
                    current_seg = Segment(
                        start=current_seg.start,
                        end=segment.end
                    )
                else:
                    filtered_segments.append((current_seg, current_track, label))
                    current_seg = segment
                    current_track = track
        
        if current_seg is not None:
            filtered_segments.append((current_seg, current_track, label))
    
    # æ™‚é–“é †ã«ã‚½ãƒ¼ãƒˆ
    filtered_segments.sort(key=lambda x: x[0].start)
    return filtered_segments

# â”€â”€â”€ 5. ASR + Diarization çµ±åˆ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def assign_speakers_to_words(asr_result: Dict[str, Any], diarization) -> Dict[str, Any]:
    """ASRçµæœã«è©±è€…æƒ…å ±ã‚’å‰²ã‚Šå½“ã¦"""
    if diarization is None:
        return asr_result
    
    try:
        import whisperx
        import numpy as np
        from pyannote.core import Segment
        
        # è©±è€…ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’æ™‚é–“é †ã«ã‚½ãƒ¼ãƒˆ
        segments = []
        for segment, track, label in diarization:
            segments.append((segment, track, label))
        segments.sort(key=lambda x: x[0].start)
        
        # è©±è€…ãƒ©ãƒ™ãƒ«ã®æ­£è¦åŒ–ï¼ˆSPEAKER_1, SPEAKER_2, ...ï¼‰
        speaker_map = {}
        for _, _, label in segments:
            if label not in speaker_map:
                speaker_map[label] = f"SPEAKER_{len(speaker_map) + 1}"
        
        # å„ASRã‚»ã‚°ãƒ¡ãƒ³ãƒˆã«è©±è€…ã‚’å‰²ã‚Šå½“ã¦
        for segment in asr_result["segments"]:
            start_time = segment["start"]
            end_time = segment["end"]
            
            # æ™‚é–“ãŒé‡ãªã‚‹è©±è€…ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’æ¢ã™
            overlapping_speakers = []
            for spk_seg, _, label in segments:
                if (spk_seg.start <= end_time and spk_seg.end >= start_time):
                    overlap = min(end_time, spk_seg.end) - max(start_time, spk_seg.start)
                    overlapping_speakers.append((label, overlap))
            
            if overlapping_speakers:
                # æœ€ã‚‚é‡ãªã‚ŠãŒå¤§ãã„è©±è€…ã‚’é¸æŠ
                speaker = max(overlapping_speakers, key=lambda x: x[1])[0]
                segment["speaker"] = speaker_map[speaker]
            else:
                # æœ€ã‚‚è¿‘ã„è©±è€…ã‚’æ¢ã™
                closest_speaker = None
                min_distance = float('inf')
                for spk_seg, _, label in segments:
                    distance = min(abs(spk_seg.end - start_time), abs(spk_seg.start - end_time))
                    if distance < min_distance:
                        min_distance = distance
                        closest_speaker = label
                
                if closest_speaker and min_distance < 0.5:  # 0.5ç§’ä»¥å†…ã®è·é›¢
                    segment["speaker"] = speaker_map[closest_speaker]
                else:
                    segment["speaker"] = "SPEAKER_UNKNOWN"
        
        print("âœ“ è©±è€…-å˜èª å‰²ã‚Šå½“ã¦å®Œäº†")
        return asr_result
        
    except Exception as e:
        print(f"âš ï¸  è©±è€…å‰²ã‚Šå½“ã¦å¤±æ•—: {e}")
        print("   è©±è€…åˆ†é›¢ãªã—ã®çµæœã‚’ä½¿ç”¨ã—ã¾ã™")
        return asr_result

def process_audio(wav_file: pathlib.Path, language: Optional[str] = None) -> Dict[str, Any]:
    """éŸ³å£°å‡¦ç†ã®ãƒ¡ã‚¤ãƒ³ãƒ•ãƒ­ãƒ¼"""
    # 1. éŸ³å£°èªè­˜ + ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆ
    asr_result = transcribe_and_align(wav_file, language)
    
    # 2. è©±è€…åˆ†é›¢
    diarization = diarize_audio(wav_file)
    
    # 3. çµæœçµ±åˆ
    final_result = assign_speakers_to_words(asr_result, diarization)
    
    return final_result

# â”€â”€â”€ 6. Markdown ç”Ÿæˆ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def generate_markdown(result: Dict[str, Any], original_url: str) -> str:
    """è»¢å†™çµæœã‚’Markdownå½¢å¼ã§ç”Ÿæˆ"""
    lines = [
        f'source: "{original_url}"',
        "",
        "## Transcript",
        ""
    ]
    
    if not result.get("segments"):
        lines.extend(["> **Warning**: è»¢å†™çµæœãŒç©ºã§ã™", ""])
        return "\n".join(lines)
    
    current_speaker = None
    current_text = []
    
    # ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’æ™‚é–“é †ã«ã‚½ãƒ¼ãƒˆ
    segments = sorted(result["segments"], key=lambda x: x["start"])
    
    for segment in segments:
        speaker = segment.get("speaker", "SPEAKER_UNKNOWN")
        text = segment.get("text", "").strip()
        
        if not text:
            continue
        
        if speaker == current_speaker:
            # åŒã˜è©±è€…ã®å ´åˆã¯æ–‡ç« ã‚’é€£çµ
            current_text.append(text)
        else:
            # è©±è€…ãŒå¤‰ã‚ã£ãŸå ´åˆã€å‰ã®ç™ºè©±ã‚’å‡ºåŠ›
            if current_speaker is not None and current_text:
                lines.append(f"> **{current_speaker}**: {' '.join(current_text)}")
            
            # æ–°ã—ã„è©±è€…ã®ç™ºè©±ã‚’é–‹å§‹
            current_speaker = speaker
            current_text = [text]
    
    # æœ€å¾Œã®ç™ºè©±ã‚’å‡ºåŠ›
    if current_speaker is not None and current_text:
        lines.append(f"> **{current_speaker}**: {' '.join(current_text)}")
    
    lines.append("")
    return "\n".join(lines)

# â”€â”€â”€ 7. ãƒ¡ã‚¤ãƒ³å‡¦ç† â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def validate_youtube_url(url: str) -> bool:
    """YouTube URLã®å¦¥å½“æ€§ã‚’ãƒã‚§ãƒƒã‚¯"""
    youtube_patterns = [
        r"https?://(www\.)?youtube\.com/watch\?v=[\w-]+",
        r"https?://(www\.)?youtube\.com/shorts/[\w-]+",
        r"https?://youtu\.be/[\w-]+",
        r"https?://m\.youtube\.com/watch\?v=[\w-]+",
    ]
    
    return any(re.match(pattern, url) for pattern in youtube_patterns)

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("ğŸ¯ YouTube Shorts â†’ Obsidian Markdown å¤‰æ›ãƒ„ãƒ¼ãƒ«")
    print(f"ğŸ“ ä¿å­˜å…ˆ: {TARGET}")
    print()
    
    # URLå…¥åŠ›
    url = input("YouTube URL ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„: ").strip()
    
    if not url:
        print("âŒ URLãŒå…¥åŠ›ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        sys.exit(1)
    
    if not validate_youtube_url(url):
        print("âŒ æœ‰åŠ¹ãªYouTube URLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
        print("   å¯¾å¿œå½¢å¼: youtube.com/watch, youtube.com/shorts, youtu.be")
        sys.exit(1)
    
    try:
        with tempfile.TemporaryDirectory() as temp_dir:
            temp_path = pathlib.Path(temp_dir)
            
            # 1. éŸ³å£°ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
            print("â¬‡ï¸  éŸ³å£°ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...")
            wav_file, video_info = download_wav(url, temp_path)
            
            video_duration = video_info.get('duration', 0)
            if video_duration > 300:  # 5åˆ†ä»¥ä¸Šã®å ´åˆè­¦å‘Š
                print(f"âš ï¸  å‹•ç”»ãŒé•·ã„ã§ã™ ({video_duration}ç§’)ã€‚å‡¦ç†ã«æ™‚é–“ãŒã‹ã‹ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
            
            # 2. éŸ³å£°å‡¦ç†ï¼ˆASR + è©±è€…åˆ†é›¢ï¼‰
            print("ğŸ™  éŸ³å£°èªè­˜ãƒ»è©±è€…åˆ†é›¢ä¸­...")
            result = process_audio(wav_file, LANG_OPT)
            
            # 3. Markdownç”Ÿæˆ
            markdown_content = generate_markdown(result, url)
            
            # 4. ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
            timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
            video_id = video_info.get('id', 'unknown')
            output_filename = f"{timestamp}_{video_id}.md"
            output_path = TARGET / output_filename
            
            output_path.write_text(markdown_content, encoding="utf-8")
            
            print()
            print("âœ… å‡¦ç†å®Œäº†!")
            print(f"ğŸ“„ ä¿å­˜å…ˆ: {output_path}")
            
            # çµæœã‚µãƒãƒªãƒ¼è¡¨ç¤º
            segment_count = len(result.get("segments", []))
            speakers = set(seg.get("speaker", "UNKNOWN") for seg in result.get("segments", []))
            print(f"ğŸ“Š ã‚»ã‚°ãƒ¡ãƒ³ãƒˆæ•°: {segment_count}")
            print(f"ğŸ‘¥ æ¤œå‡ºè©±è€…æ•°: {len(speakers)} ({', '.join(sorted(speakers))})")
            
    except KeyboardInterrupt:
        print("\nâš ï¸  å‡¦ç†ãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main()